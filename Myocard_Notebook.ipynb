{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AMI with the new dataset\n",
    "The pipeline is run with all the columns of the dataset, we are only keeping 10 features selected with the SelectkBest method in order to do the PCA with fewer number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First study : supervised prediction\n",
    "At first with the existing pipeline created during the pancreas cancer study we will try to use supervised learning in order to predict the relapse of AMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the librairies and the already written functions that we will use later on\n",
    "%run \"2 - General\\List functions.ipynb\"   ## List of general functions that we will need such as visualization, training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the data from the company Onedrive\n",
    "raw_data = pd.read_excel(r'C:/Users/33753/OneDrive - Numa Health/Documents - 8 - Medical & Data Science/6 - Théo/01 - Data/6 - Myocarde/df_myocarde_230420_reduced.xlsx',\n",
    "                     )\n",
    "raw_data.set_index('Unnamed: 0', inplace= True) \n",
    "\n",
    "# Removing all the last columns corresponding to the outcomes\n",
    "data = raw_data.loc[:,raw_data.columns[:raw_data.shape[1]-8]]\n",
    "outcome = 'MACE 2 yr (death, stroke, MI, hositalizacija(stento restenoz, new stenoz, SN))'\n",
    "data[outcome] = raw_data.loc[:,outcome]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Renaming the columns with standard names\n",
    "bio = {'RBC (I) e6': 'RBC',\n",
    " 'WBC e3 (I)skyriuje' : 'WBC',\n",
    " 'PMN (%) I' : 'PMN',\n",
    " 'LYMPH (%)I' : 'LYMPH',\n",
    " 'Mono (%) I' : 'MONO',\n",
    " 'EOS (%) I' : 'EOS',\n",
    " 'BASO (%) I' : 'BASO',\n",
    " 'PLT.(I)' : 'PLT',\n",
    " 'Hb mg/dL (I) ' : 'HG',\n",
    " 'HCT (I)' : 'HCT',\n",
    " 'MCH (I)' : 'MHC',\n",
    " 'MCHC (I)' : 'MCHC',\n",
    " 'K (I)' : 'K',\n",
    " 'Ca (I)' : 'CA'}\n",
    "\n",
    "data.rename(columns = bio, inplace = True)\n",
    "\n",
    "# Dropping the unused endobiogenical indexes\n",
    "data.drop(columns = ['Unnamed: 0.1','Cortisol INDEX','Mono/Lym','G/T','Adaptation'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking if each column is in the correct unit to calculate the endobiological indexes based on a function specific for each biomarkers\n",
    "## If not in the correct unit transforming the data\n",
    "data = unit_check(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculation of the endobiological indexes in their raw form (numerical)\n",
    "calculation_raw_index(data, list_index = [] , Basophile = 'BASO', Eosinophile = 'EOS', Hemoglobine = 'HG', Leucocyte = 'WBC',\n",
    "                    Lymphocyte = 'LYMPH', Monocyte = 'MONO', Neutrophile = 'PMN', Platelet = 'PLT', TCMHemog = 'MHC',\n",
    "                    calcium ='CA', potassium = 'K',hematies='RBC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columns to be removed from the dataset based on Kamyar (Numas's endobiogeny specialist and physician) choice (27/04/2023)\n",
    "cols_to_remove = ['STEMI/ non STEMI.1' ,'BMI (kg/m2)' ,'Dyslipidemia' ,'Hypertesion' ,'Smoking' ,\n",
    "        'DM' ,'CABG' ,'History of IHD' ,'History of heart failue' ,\n",
    "        'previous PCI' ,'Previous MI' ,'Risk AGG' ,'EF post PCI CAT; <=45 = 1' ,\n",
    "        'Culprit vessel: 0=RCA, 1=Rt Circumflex, 2=LAD' ,'Number of diseased vessel' ,'percentage of culpret vessel; >=50 significant if other physiologic variables are present' ,\n",
    "        'Coronary dominancy (the artery that feeds the posterior descending artery): 40% RCA, 60% left Circumflex)' ,\n",
    "        'Cardiac rythm during incharge' ,'in-hospital II - III* AVB' ,'SV during hosp. ' ,'Time until death (1yr)/ Laikas iki mirties (1m)' ,\n",
    "        'Died in 1 yr / Mire per 1m.' ,'Time until death (2yr)/ Laikas iki mirties (2m)' ,'Na (I)' ,'eGFR 1 (GFG)' ,'Cr 1 (mmol/l)' ,'Cortisol INDEX' ,\n",
    "        'Death in hospital period; 0=No, 1=Yes' ,'1yr MACE( death. MI. stroke, hospitalisation)' ,'MACE 03.28']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creation of new columns based on Kamyar's insight\n",
    "data['Mono/Lym']=data['MONO']/data['LYMPH']\n",
    "data['Thyroid relaunching corrected'] = data['Mono/Lym']*data['genit_thyro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition of the useful features that will be used for the study\n",
    "feature_biomarkers_4 = ['WBC','HG', 'RBC', 'MHC', 'PMN', 'EOS', 'BASO', 'LYMPH', 'MONO', 'PLT',\n",
    "                     'eGFR 1 CAT 0 = >90', '?? Troponin [Trop MAX]','All SERUM Cortisol Kortizolis-Serum','CRP']\n",
    "feature_index_4 = ['IML', 'IMP', 'cortisol_func', 'cortisol_struc', 'consum_aggr_index', 'pro_inflam',\n",
    "                'adapt_ratio', 'Mono/Lym', 'genit_thyro', 'Thyroid relaunching corrected']\n",
    "feature_other_4 = list(\n",
    "    filter(lambda x: x not in feature_biomarkers_4 and x not in feature_index_4, data.columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running the classification pipeline previously created on the outcome : MACE 2yr \n",
    "feature_sets = {'feature_biomarkers':feature_biomarkers_4, \n",
    "                'feature_index':feature_index_4, \n",
    "                'feature_other':feature_other_4}\n",
    "\n",
    "# Specifying the date to calculate the time taken for the whole process\n",
    "today = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M')\n",
    "\n",
    "for name, feature_set in feature_sets.items():\n",
    "    df = data.loc[~data[outcome].isna()].copy()\n",
    "    for col in cols_to_remove:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns = col, inplace=True)\n",
    "    if 'Culprit vessel: 0=RCA, 1=Rt Circumflex, 2=LAD' in df.columns:\n",
    "        df.drop(columns = ['Culprit vessel: 0=RCA, 1=Rt Circumflex, 2=LAD'], inplace=True)\n",
    "    if outcome in feature_set:\n",
    "        feature_set.remove(outcome)\n",
    "\n",
    "    # Creating the path where all the results, visualization etc will be stored\n",
    "    path = r'C:/Users/33753/OneDrive - Numa Health/Documents - 8 - Medical & Data Science/6 - Théo/03 - Research/5 - Pathologies/6-Myocarde/'\n",
    "    path += today+'/'+'MACE 2yr'+'/'+name+'/'\n",
    "\n",
    "    pipeline_classification(df, feature_set, col_to_pred=outcome, cutoff=0, thresh=None, save=True, outdir=path, show=False,\n",
    "                                scoring='accuracy', loss_function=None, dict_models=None, dict_param_models=None, normalization_method=None,\n",
    "                                imputation_method='iterative', feature_selection_method='Voting', n_features=4, calcul='No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re-Running the classification pipeline with the same outcome but with updating features based on the previous results\n",
    "final_features = ['PMN','EOS','LYMPH','immed_adapt_score','cortisol_struc','Thyroid relaunching corrected','genit_thyro','IML','IMP','adapt_ratio'\n",
    "                '6 month Stroke','Killip klasė (CHF)','heart rate and conduction disarrangement during hospitalization']\n",
    "\n",
    "df = data.loc[~data[outcome].isna()].copy()\n",
    "if 'Culprit vessel: 0=RCA, 1=Rt Circumflex, 2=LAD' in df.columns:\n",
    "    df.drop(columns = ['Culprit vessel: 0=RCA, 1=Rt Circumflex, 2=LAD'], inplace=True)\n",
    "    \n",
    "# Creating the path where all the results, visualization etc will be stored\n",
    "path = r'C:/Users/33753/OneDrive - Numa Health/Documents - 8 - Medical & Data Science/6 - Théo/03 - Research/5 - Pathologies/6-Myocarde/'\n",
    "path += today+'/'+'MACE 2yr'+'/'+'final_features'+'/'\n",
    "\n",
    "pipeline_classification(df, final_features, col_to_pred=outcome, cutoff=0, thresh=None, save=True, outdir=path, show=False,\n",
    "                            scoring='accuracy', loss_function=None, dict_models=None, dict_param_models=None, normalization_method=None,\n",
    "                            imputation_method='iterative', feature_selection_method='Voting', n_features=6, calcul='No')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second study : AMI with double model and PCA\n",
    "As the fist study wasn't probing we tried a new method with unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the librairies and the already written functions that we will use later on\n",
    "%run \"C:\\Users\\33753\\Documents\\Machine-Learning\\1-Functions\\List_Functions.ipynb\" ## List of general functions that we will need such as visualization, training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the path to save all the plot, datasets etc\n",
    "path = r'C:/Users/33753/OneDrive - Numa Health/Documents - 8 - Medical & Data Science/6 - Théo/03 - Research/05 - Pathologies/06-Myocarde/'\n",
    "path += today+'/'+'Test PCA'+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Myocarde dataset                  \n",
    "data = pd.read_excel(r'C:/Users/33753/OneDrive - Numa Health/Documents - 8 - Medical & Data Science/6 - Théo/01 - Data/6 - Myocarde/df_myocarde_230420_reduced.xlsx',\n",
    "                     )\n",
    "data.set_index('Unnamed: 0', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the name of all the useful features and the outcome\n",
    "AMI_features = ['RBC (I) e6', 'WBC e3 (I)skyriuje', 'PMN (%) I',\n",
    "       'LYMPH (%)I', 'Mono (%) I', 'EOS (%) I', 'Cortisol INDEX', 'Mono/Lym',\n",
    "       'G/T', 'Adaptation', 'BASO (%) I', 'PLT.(I)', 'Hb mg/dL (I) ',\n",
    "       'MCH (I)', 'Na (I)', '?? Troponin [Trop MAX]',\n",
    "       'All SERUM Cortisol Kortizolis-Serum', 'FT3 (I)', 'First TnI',\n",
    "       'Total Cholesterol', 'eGFR 1 (GFG)', 'eGFR 1 CAT 0 = >90',\n",
    "       'Cr 1 (mmol/l)', 'CRB', 'Hospitalisation Period (days)', 'Age',\n",
    "       'Age CAT <65 = 0;', 'Sex: 0=MALE, 1=FEMALE', 'STEMI/ non STEMI',\n",
    "       'STEMI/ non STEMI.1', 'Killip klasė (CHF)', 'BMI (kg/m2)',\n",
    "       'BMI CAT >30=1', 'Dyslipidemia', 'Hypertesion', 'Smoking', 'DM',\n",
    "       'Familial IHD', 'CABG', 'History of IHD', 'History of heart failue',\n",
    "       'previous PCI', 'Previous MI', 'Risk AGG',\n",
    "       'EF after 24 hr from PCI >=50 NL, <=45 is increased risk of CHF, Mortality',\n",
    "       'EF post PCI CAT; <=45 = 1',\n",
    "       'Culprit vessel: 0=RCA, 1=Rt Circumflex, 2=LAD',\n",
    "       'Number of diseased vessel',\n",
    "       'percentage of culpret vessel; >=50 significant if other physiologic variables are present',\n",
    "       'Coronary dominancy (the artery that feeds the posterior descending artery): 40% RCA, 60% left Circumflex)',\n",
    "       'heart rate and conduction disarrangement during hospitalization',\n",
    "       'Cardiac rythm during incharge', 'in hospital AFib', 'SV during hosp. ',\n",
    "       'CRP', '6 month Stroke']\n",
    "outcome = 'MACE 2 yr (death, stroke, MI, hositalizacija(stento restenoz, new stenoz, SN))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary to rename all the biomarkers to a standard naming\n",
    "bio = {'RBC (I) e6': 'RBC',\n",
    " 'WBC e3 (I)skyriuje' : 'WBC',\n",
    " 'PMN (%) I' : 'PMN',\n",
    " 'LYMPH (%)I' : 'LYMPH',\n",
    " 'Mono (%) I' : 'MONO',\n",
    " 'EOS (%) I' : 'EOS',\n",
    " 'BASO (%) I' : 'BASO',\n",
    " 'PLT.(I)' : 'PLT',\n",
    " 'Hb mg/dL (I) ' : 'HB',\n",
    " 'HCT (I)' : 'HCT',\n",
    " 'MCH (I)' : 'MHC',\n",
    " 'MCHC (I)' : 'MCHC',\n",
    " 'K (I)' : 'K',\n",
    " 'Ca (I)' : 'CA'}\n",
    "\n",
    "data.rename(columns = bio, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the features to be removed according to the insight of Kamyar Hedayat\n",
    "toBeRemovedKamyar = ['STEMI/ non STEMI.1' ,'BMI (kg/m2)' ,'Dyslipidemia' ,'Hypertesion' ,'Smoking' ,\n",
    "        'DM' ,'CABG' ,'History of IHD' ,'History of heart failue' ,\n",
    "        'previous PCI' ,'Previous MI' ,'Risk AGG' ,'EF post PCI CAT; <=45 = 1' ,\n",
    "        'Culprit vessel: 0=RCA, 1=Rt Circumflex, 2=LAD' ,'Number of diseased vessel' ,'percentage of culpret vessel; >=50 significant if other physiologic variables are present' ,\n",
    "        'Coronary dominancy (the artery that feeds the posterior descending artery): 40% RCA, 60% left Circumflex)' ,\n",
    "        'Cardiac rythm during incharge','SV during hosp. ' ,'Time until death (1yr)/ Laikas iki mirties (1m)' ,\n",
    "        'Died in 1 yr / Mire per 1m.' ,'Time until death (2yr)/ Laikas iki mirties (2m)' ,'Na (I)' ,'eGFR 1 (GFG)' ,'Cr 1 (mmol/l)' ,'Cortisol INDEX' ,\n",
    "        'Death in hospital period; 0=No, 1=Yes' ,'1yr MACE( death. MI. stroke, hospitalisation)' ,'MACE 03.28']\n",
    "data.drop(columns=toBeRemovedKamyar, inplace=True)\n",
    "\n",
    "# List of features to removed to avoid duplicates\n",
    "data.drop(columns = ['Unnamed: 0.1','Mono/Lym','G/T','Adaptation'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing of the data\n",
    "df, new_feature_set = engineering(data, AMI_features, col_to_pred=outcome, cutoff=0, normalization_method=None, imputation_method='iterative',\n",
    "                                    feature_selection_method = None, n_features=5, calcul='Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X : the dataset without the expected outcome\n",
    "X = df.copy()\n",
    "X.drop(columns=[outcome], inplace=True)\n",
    "X_index = X.index\n",
    "\n",
    "# PCA\n",
    "# Apply PCA to reduce the dimensionality of the data to n_features\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pca.explained_variance_ : \\n\", pca.explained_variance_, '\\n')\n",
    "print(\"pca.explained_variance_ratio_ :\\n\", pca.explained_variance_ratio_, '\\n')\n",
    "print(\"pca.explained_variance_ratio_.cumsum() :\\n\", pca.explained_variance_ratio_.cumsum(), '\\n')\n",
    "print(\"pca.noise_variance_ :\\n\", pca.noise_variance_, '\\n')\n",
    "print(\"pca.singular_values_ :\\n\", pca.singular_values_, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the explained variance ratio cumulated with lines to better view the 90% threshold\n",
    "plt.barh(y = np.arange(1, len(list(pca.explained_variance_ratio_.cumsum()))+1), width = list(pca.explained_variance_ratio_.cumsum()), )\n",
    "plt.axhline(1, color = 'red')\n",
    "plt.axvline(0.90, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of facotrial axes to keep\n",
    "nbFactAxe = len([x for x in list(pca.explained_variance_ratio_.cumsum()) if x < 0.90]) + 1\n",
    "if nbFactAxe < 5:\n",
    "    nbFactAxe = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the most important features from the PCA and saving them\n",
    "mostImportantFeatures = pd.DataFrame()\n",
    "i=1\n",
    "index = range(len(pca.components_[0]))\n",
    "for comp in range(nbFactAxe):\n",
    "    s = sorted(index, reverse=True, key=lambda i: abs(pca.components_[comp])[i])[:nbFactAxe]\n",
    "    S = sorted(abs(pca.components_[comp]), reverse=True)[:nbFactAxe]\n",
    "    mostImportantFeatures[i] = list(df.columns[s])\n",
    "    i+=1\n",
    "save_dataframe(outdir = path, dataframe=mostImportantFeatures, file_type='xlsx', name = 'Most important features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the most recurrent features\n",
    "value_counts = mostImportantFeatures.values.flatten().tolist()\n",
    "count = pd.Series(value_counts).value_counts()\n",
    "mostReccurentFeatures = pd.DataFrame(count)\n",
    "save_dataframe(outdir = path, dataframe=mostReccurentFeatures, file_type='xlsx', name = 'Occurences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first new variable is explaining more than 90% of the variance of the data which means that the first variable have more than 90% of the information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's only keep the 11 first variables\n",
    "fact_ax = list(np.arange(0,nbFactAxe))\n",
    "dfReduced = pd.DataFrame(X).loc[:,fact_ax]\n",
    "dfReduced['index']=X_index\n",
    "dfReduced = dfReduced.set_index('index')\n",
    "dfReduced[outcome] = df.loc[:,outcome]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization of after preprocessing data\n",
    "vizualisation(dfReduced, fact_ax, outcome, save=True, outdir=path+'After_Preprocessing/', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "X, y = dfReduced.loc[:,list(dfReduced.columns[:-1])], dfReduced.loc[:,outcome]\n",
    "\n",
    "# Fitting the Decision Tree\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model = dt_model.fit(X,y)\n",
    "\n",
    "# Fitting the Fandom Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model = rf_model.fit(X,y)\n",
    "\n",
    "# Fitting the XGBoost\n",
    "xgb_model = XGBClassifier(eval_metric='mlogloss')\n",
    "xgb_model = xgb_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the results\n",
    "dict_result = {'DecisionTree':{},\n",
    "               'RandomForest':{},\n",
    "               'XGBoost':{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the variable that will store the model trained on the survival population\n",
    "dt_model_survival = DecisionTreeClassifier()\n",
    "rf_model_survival = RandomForestClassifier()\n",
    "xgb_model_survival = XGBClassifier(eval_metric='mlogloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StratifiedKFold with 10 splits\n",
    "skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize an empty list to store the residual errors\n",
    "residual_errors_dt = []\n",
    "residual_errors_rf = []\n",
    "residual_errors_xgb = []\n",
    "\n",
    "# Empty dataframe to store the wrong predictions\n",
    "dict_error_analysis = {'DecisionTree':[],\n",
    "               'RandomForest':[],\n",
    "               'XGBoost':[]}\n",
    "\n",
    "iter = 1\n",
    "\n",
    "# Iterate over each fold\n",
    "for train_index, test_index in skfold.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Showing the distribution of death and survival in each iteration\n",
    "    display_distribution_pie(df.iloc[train_index], outcome, save = True,\n",
    "                             outdir=path+'iter_'+str(iter)+'/', show=False)\n",
    "    display_distribution_pie(df.iloc[test_index], outcome, save = True,\n",
    "                             outdir=path+'iter_'+str(iter)+'/', show=False)\n",
    "\n",
    "    # Initialize and fit the decision tree model\n",
    "    dt_model_CV = DecisionTreeClassifier()\n",
    "    rf_model_CV = RandomForestClassifier()\n",
    "    xgb_model_CV = XGBClassifier(eval_metric='mlogloss')\n",
    "    \n",
    "    dt_model_CV = dt_model_CV.fit(X_train, y_train)\n",
    "    rf_model_CV = rf_model_CV.fit(X_train, y_train)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        xgb_model_CV = xgb_model_CV.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_dt = dt_model_CV.predict(X_test)\n",
    "    y_pred_rf = rf_model_CV.predict(X_test)\n",
    "    y_pred_xgb = xgb_model_CV.predict(X_test)\n",
    "\n",
    "\n",
    "    # Training a second model on the survival population to determine if a hidden pattern exists\n",
    "    X_train_survival = X_test.copy()\n",
    "    X_train_survival['Outcome'] = y_test\n",
    "\n",
    "    X_train_survival['PredictedOutcomeDt'] = y_pred_dt\n",
    "    X_train_survival['PredictedOutcomeRf'] = y_pred_rf\n",
    "    X_train_survival['PredictedOutcomeXgb'] = y_pred_xgb\n",
    "\n",
    "    X_train_survival_red = X_train_survival.loc[X_train_survival['Outcome']==0]\n",
    "    y_test_survival_dt = X_train_survival_red['PredictedOutcomeDt']\n",
    "    y_test_survival_rf = X_train_survival_red['PredictedOutcomeRf']\n",
    "    y_test_survival_xgb = X_train_survival_red['PredictedOutcomeXgb']\n",
    "\n",
    "    dt_model_survival = dt_model_survival.fit(X_train_survival_red.loc[:,fact_ax], y_test_survival_dt)\n",
    "    rf_model_survival = rf_model_survival.fit(X_train_survival_red.loc[:,fact_ax], y_test_survival_rf)\n",
    "    xgb_model_survival = xgb_model_survival.fit(X_train_survival_red.loc[:,fact_ax], y_test_survival_xgb)\n",
    "\n",
    "\n",
    "    # Adding the index of the wrong predictions to the corresponding dataframe\n",
    "    dict_error_analysis['DecisionTree'] += list(X_train_survival[X_train_survival['Outcome'] != X_train_survival['PredictedOutcomeDt']].index)\n",
    "    dict_error_analysis['RandomForest'] += list(X_train_survival[X_train_survival['Outcome'] != X_train_survival['PredictedOutcomeRf']].index)\n",
    "    dict_error_analysis['XGBoost'] += list(X_train_survival[X_train_survival['Outcome'] != X_train_survival['PredictedOutcomeXgb']].index)\n",
    "\n",
    "    # Calculate the residual error (e.g., mean squared error)\n",
    "    residual_error_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "    residual_errors_dt.append(round(residual_error_dt, 4))\n",
    "\n",
    "    residual_error_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "    residual_errors_rf.append(round(residual_error_rf,4))\n",
    "\n",
    "    residual_error_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "    residual_errors_xgb.append(round(residual_error_xgb,4))\n",
    "\n",
    "    # Plotting the confusion matrix for each model\n",
    "    metrics_dt = display_confusion_matrix(X_test, y_test, dt_model_CV, name=None, y_pred=y_pred_dt, return_metrics=True, save = True,\n",
    "                             outdir=path+'iter_'+str(iter)+'/', show=False)\n",
    "    metrics_rf = display_confusion_matrix(X_test, y_test, rf_model_CV, name=None, y_pred=y_pred_rf, return_metrics=True, save = True,\n",
    "                             outdir=path+'iter_'+str(iter)+'/', show=False)\n",
    "    metrics_xgb = display_confusion_matrix(X_test, y_test, y_pred_xgb, name=None, y_pred=y_pred_xgb, return_metrics=True, save = True,\n",
    "                             outdir=path+'iter_'+str(iter)+'/', show=False)\n",
    "    \n",
    "    # Saving the results in the dictionary\n",
    "    dict_result['DecisionTree'][str(iter)] = {'MSE' : round(residual_error_dt,4),\n",
    "                                    'SB' : round(metrics_dt[1],4),\n",
    "                                    'ST' : round(metrics_dt[2],4)}\n",
    "    dict_result['RandomForest'][str(iter)] = {'MSE' : round(residual_error_rf,4),\n",
    "                                    'SB' : round(metrics_rf[1],4),\n",
    "                                    'ST' : round(metrics_rf[2],4)}\n",
    "    dict_result['XGBoost'][str(iter)] = {'MSE' : round(residual_error_xgb,4),\n",
    "                                    'SB' : round(metrics_xgb[1],4),\n",
    "                                    'ST' : round(metrics_xgb[2],4)}\n",
    "\n",
    "    iter+=1\n",
    "\n",
    "# Print the residual error for the current fold and the average residual error across all folds\n",
    "print(\"Residual error (fold):\", residual_errors_dt)\n",
    "print(\"Average residual error:\", round(sum(residual_errors_dt) / len(residual_errors_dt),4))\n",
    "\n",
    "# Print the residual error for the current fold and the average residual error across all folds\n",
    "print(\"Residual error (fold):\", residual_errors_rf)\n",
    "print(\"Average residual error:\", round(sum(residual_errors_rf) / len(residual_errors_rf),4))\n",
    "\n",
    "# Print the residual error for the current fold and the average residual error across all folds\n",
    "print(\"Residual error (fold):\", residual_errors_xgb)\n",
    "print(\"Average residual error:\", round(sum(residual_errors_xgb) / len(residual_errors_xgb),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the rows that were falsely predicted\n",
    "save_dataframe(outdir=path+'Error analysis/', dataframe=data.loc[dict_error_analysis['DecisionTree'],:], file_type = 'xlsx', name='Error_Analysis_CV_dt')\n",
    "save_dataframe(outdir=path+'Error analysis/', dataframe=data.loc[dict_error_analysis['RandomForest'],:], file_type = 'xlsx', name='Error_Analysis_CV_rf')\n",
    "save_dataframe(outdir=path+'Error analysis/', dataframe=data.loc[dict_error_analysis['XGBoost'],:], file_type = 'xlsx', name='Error_Analysis_CV_xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean value and the STD of all the metrics\n",
    "df_AVG = pd.DataFrame()\n",
    "for typeModel in ['DecisionTree','RandomForest','XGBoost']:\n",
    "    MSE_list = []\n",
    "    SB_list=[]\n",
    "    ST_list=[]\n",
    "    for value in dict_result[typeModel].values():\n",
    "        MSE_list.append(value['MSE'])\n",
    "        SB_list.append(value['SB'])\n",
    "        ST_list.append(value['ST'])\n",
    "    df_AVG.loc['AVG',typeModel] = str({'MSE':round(np.mean(MSE_list),4),\n",
    "                                'SB':round(np.mean(SB_list),4),\n",
    "                                'ST':round(np.mean(ST_list),4)})\n",
    "    df_AVG.loc['STD',typeModel] = str({'MSE':round(np.std(MSE_list),4),\n",
    "                                'SB':round(np.std(SB_list),4),\n",
    "                                'ST':round(np.std(ST_list),4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results as a dataframe\n",
    "df_result = pd.DataFrame(dict_result)\n",
    "df_result = pd.concat([df_result,df_AVG])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LeaveOneOUt cross validation\n",
    "skfold = LeaveOneOut()\n",
    "\n",
    "# Initialize an empty list to store the residual errors\n",
    "residual_errors_dt = []\n",
    "residual_errors_rf = []\n",
    "residual_errors_xgb = []\n",
    "\n",
    "dict_PredictedOutcome = {'DecisionTree':[],\n",
    "                        'RandomForest':[],\n",
    "                        'XGBoost':[]}\n",
    "\n",
    "pred_test = pd.DataFrame()\n",
    "dict_error_analysis = {'DecisionTree':[],\n",
    "               'RandomForest':[],\n",
    "               'XGBoost':[]}\n",
    "\n",
    "# Iterate over each fold\n",
    "for train_index, test_index in skfold.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Initialize and fit the decision tree model\n",
    "    dt_model_CV = DecisionTreeClassifier()\n",
    "    rf_model_CV = RandomForestClassifier()\n",
    "    xgb_model_CV = XGBClassifier(eval_metric='mlogloss')\n",
    "    \n",
    "    dt_model_CV = dt_model_CV.fit(X_train, y_train)\n",
    "    rf_model_CV = rf_model_CV.fit(X_train, y_train)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        xgb_model_CV = xgb_model_CV.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_dt = dt_model_CV.predict(X_test)\n",
    "    dict_PredictedOutcome['DecisionTree'].append(y_pred_dt)\n",
    "    y_pred_rf = rf_model_CV.predict(X_test)\n",
    "    dict_PredictedOutcome['RandomForest'].append(y_pred_rf)\n",
    "    y_pred_xgb = xgb_model_CV.predict(X_test)\n",
    "    dict_PredictedOutcome['XGBoost'].append(y_pred_xgb)\n",
    "    pred_test = pd.concat([pred_test,y_test])\n",
    "\n",
    "    # Calculate the residual error (e.g., mean squared error)\n",
    "    residual_error_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "    residual_errors_dt.append(residual_error_dt)\n",
    "\n",
    "    residual_error_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "    residual_errors_rf.append(residual_error_rf)\n",
    "\n",
    "    residual_error_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "    residual_errors_xgb.append(residual_error_xgb)\n",
    "\n",
    "pred_test.loc[:,'PredictedOutcomeDt'] = [int(arr[0]) for arr in dict_PredictedOutcome['DecisionTree']]\n",
    "pred_test.loc[:,'PredictedOutcomeRf'] = [int(arr[0]) for arr in dict_PredictedOutcome['RandomForest']]\n",
    "pred_test.loc[:,'PredictedOutcomeXgb'] = [int(arr[0]) for arr in dict_PredictedOutcome['XGBoost']]\n",
    "\n",
    "# Adding the index of the wrong predictions to the corresponding dataframe\n",
    "dict_error_analysis['DecisionTree'] += list(pred_test[pred_test[0] != pred_test['PredictedOutcomeDt']].index)\n",
    "dict_error_analysis['RandomForest'] += list(pred_test[pred_test[0] != pred_test['PredictedOutcomeRf']].index)\n",
    "dict_error_analysis['XGBoost'] += list(pred_test[pred_test[0] != pred_test['PredictedOutcomeXgb']].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the rows that were falsely predicted\n",
    "save_dataframe(outdir=path+'Error analysis/', dataframe=data.loc[dict_error_analysis['DecisionTree'],:], file_type = 'xlsx', name='Error_Analysis_LOO_dt')\n",
    "save_dataframe(outdir=path+'Error analysis/', dataframe=data.loc[dict_error_analysis['RandomForest'],:], file_type = 'xlsx', name='Error_Analysis_LOO_rf')\n",
    "save_dataframe(outdir=path+'Error analysis/', dataframe=data.loc[dict_error_analysis['XGBoost'],:], file_type = 'xlsx', name='Error_Analysis_LOO_xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying a Leave One Out cross validation process\n",
    "df_LOO = pd.DataFrame()\n",
    "for typeModel in ['DecisionTree','RandomForest','XGBoost']:\n",
    "\n",
    "    # Calculate the confusion matrix for all folds\n",
    "    cm = confusion_matrix(pred_test[0], dict_PredictedOutcome[typeModel])\n",
    "\n",
    "    # Calculate sensitivity (recall)\n",
    "    sensitivity = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
    "\n",
    "    # Calculate specificity\n",
    "    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    mse = mean_squared_error(pred_test[0], dict_PredictedOutcome[typeModel])\n",
    "\n",
    "    df_LOO.loc['LOO',typeModel] = str({'MSE':round(mse,4),\n",
    "                                     'SB':round(sensitivity,4),\n",
    "                                     'ST':round(specificity,4)})\n",
    "\n",
    "    print(\"Sensitivity:\", round(sensitivity,4))\n",
    "    print(\"Specificity:\", round(specificity,4))\n",
    "    print(\"Mean Squared Error:\", round(mse,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe with all the results\n",
    "df_result = pd.concat([df_result,df_LOO])\n",
    "df_result = df_result.applymap(lambda x: str(x).replace('{', '').replace('}', ''))\n",
    "save_dataframe(outdir=path, dataframe=df_result, file_type = 'xlsx', name='results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END of PCA study"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
