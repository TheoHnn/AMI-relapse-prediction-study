{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AMI with the new dataset\n",
    "The pipeline is run with all the columns of the dataset, we are only keeping 10 features selected with the SelectkBest method in order to do the PCA with fewer number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the librairies and the already written functions that we will use later on\n",
    "%run \"2 - General\\List functions.ipynb\"   ## List of general functions that we will need such as visualization, training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specifying the date to calculate the time taken for the whole process\n",
    "today = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'C:/Users/33753/OneDrive - Numa Health/Documents - 8 - Medical & Data Science/6 - Théo/01 - Data/6 - Myocarde/df_myocarde_230420_reduced.xlsx',\n",
    "                     )\n",
    "data.set_index('Unnamed: 0', inplace= True)\n",
    "\n",
    "# Removing all the last columns corresponding to the outcomes\n",
    "data_1 = data.loc[:,data.columns[:data.shape[1]-8]]\n",
    "outcome = 'MACE 2 yr (death, stroke, MI, hositalizacija(stento restenoz, new stenoz, SN))'\n",
    "data_1[outcome] = data.loc[:,outcome]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio = {'RBC (I) e6': 'RBC',\n",
    " 'WBC e3 (I)skyriuje' : 'WBC',\n",
    " 'PMN (%) I' : 'PMN',\n",
    " 'LYMPH (%)I' : 'LYMPH',\n",
    " 'Mono (%) I' : 'MONO',\n",
    " 'EOS (%) I' : 'EOS',\n",
    " 'BASO (%) I' : 'BASO',\n",
    " 'PLT.(I)' : 'PLT',\n",
    " 'Hb mg/dL (I) ' : 'HG',\n",
    " 'HCT (I)' : 'HCT',\n",
    " 'MCH (I)' : 'MHC',\n",
    " 'MCHC (I)' : 'MCHC',\n",
    " 'K (I)' : 'K',\n",
    " 'Ca (I)' : 'CA'}\n",
    "\n",
    "data_1.rename(columns = bio, inplace = True)\n",
    "\n",
    "data_1.drop(columns = ['Unnamed: 0.1','Cortisol INDEX','Mono/Lym','G/T','Adaptation'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = unit_check(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculation_raw_index(data_1, list_index = [] , Basophile = 'BASO', Eosinophile = 'EOS', Hemoglobine = 'HG', Leucocyte = 'WBC',\n",
    "                    Lymphocyte = 'LYMPH', Monocyte = 'MONO', Neutrophile = 'PMN', Platelet = 'PLT', TCMHemog = 'MHC',\n",
    "                    calcium ='CA', potassium = 'K',hematies='RBC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_biomarkers_3 = ['WBC','HG', 'RBC', 'MHC', 'PMN', 'EOS', 'BASO', 'LYMPH', 'MONO', 'PLT']\n",
    "feature_index_3 = ['hematies','adapt_ratio', 'locus_ceru_aggr', 'IML', 'IMP', 'genit_thyro', 'loc_ceru_adaptation',\n",
    "                 'immed_adapt_index', 'cortisol_func', 'cortisol_struc', 'adr_cort_func', 'adr_cort_struc', 'allergy',\n",
    "                 'evok_hist', 'adapt_perm','immed_adapt_score', 'cata_ana', 'starter', 'alpha_symp_index',\n",
    "                 'consum_aggr_index', 'locus_ceru_index', 'androgenic', 'AC_OC', 'genital','adaptogene', 'acc_cont', 'pro_inflam']\n",
    "feature_other_3 = list(filter(lambda x: x not in feature_biomarkers_3, list(data_1.columns)))\n",
    "feature_other_3 = list(filter(lambda x: x not in feature_index_3, feature_other_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New columns to be removed based on Kamyar choice \n",
    "## 27/04/2023\n",
    "cols_to_remove = ['STEMI/ non STEMI.1' ,'BMI (kg/m2)' ,'Dyslipidemia' ,'Hypertesion' ,'Smoking' ,\n",
    "        'DM' ,'CABG' ,'History of IHD' ,'History of heart failue' ,\n",
    "        'previous PCI' ,'Previous MI' ,'Risk AGG' ,'EF post PCI CAT; <=45 = 1' ,\n",
    "        'Culprit vessel: 0=RCA, 1=Rt Circumflex, 2=LAD' ,'Number of diseased vessel' ,'percentage of culpret vessel; >=50 significant if other physiologic variables are present' ,\n",
    "        'Coronary dominancy (the artery that feeds the posterior descending artery): 40% RCA, 60% left Circumflex)' ,\n",
    "        'Cardiac rythm during incharge' ,'in-hospital II - III* AVB' ,'SV during hosp. ' ,'Time until death (1yr)/ Laikas iki mirties (1m)' ,\n",
    "        'Died in 1 yr / Mire per 1m.' ,'Time until death (2yr)/ Laikas iki mirties (2m)' ,'Na (I)' ,'eGFR 1 (GFG)' ,'Cr 1 (mmol/l)' ,'Cortisol INDEX' ,\n",
    "        'Death in hospital period; 0=No, 1=Yes' ,'1yr MACE( death. MI. stroke, hospitalisation)' ,'MACE 03.28']\n",
    "\n",
    "data_1['Mono/Lym']=data_1['MONO']/data_1['LYMPH']\n",
    "data_1['Thyroid relaunching corrected'] = data_1['Mono/Lym']*data_1['genit_thyro']\n",
    "\n",
    "feature_biomarkers_4 = ['WBC','HG', 'RBC', 'MHC', 'PMN', 'EOS', 'BASO', 'LYMPH', 'MONO', 'PLT',\n",
    "                     'eGFR 1 CAT 0 = >90', '?? Troponin [Trop MAX]','All SERUM Cortisol Kortizolis-Serum','CRP']\n",
    "feature_index_4 = ['IML', 'IMP', 'cortisol_func', 'cortisol_struc', 'consum_aggr_index', 'pro_inflam',\n",
    "                'adapt_ratio', 'Mono/Lym', 'genit_thyro', 'Thyroid relaunching corrected']\n",
    "feature_other_4 = list(filter(lambda x: x not in feature_biomarkers_4, list(data_1.columns)))\n",
    "feature_other_4 = list(filter(lambda x: x not in feature_index_4, feature_other_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MACE 2yr outcome\n",
    "\n",
    "feature_sets = {'feature_biomarkers':feature_biomarkers_4,\n",
    "                'feature_index':feature_index_4, \n",
    "                'feature_other':feature_other_4}\n",
    "today = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M')\n",
    "for name, feature_set in feature_sets.items():\n",
    "    df = data_1.loc[~data_1[outcome].isna()].copy()\n",
    "    for col in cols_to_remove:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns = col, inplace=True)\n",
    "    if 'Culprit vessel: 0=RCA, 1=Rt Circumflex, 2=LAD' in df.columns:\n",
    "        df.drop(columns = ['Culprit vessel: 0=RCA, 1=Rt Circumflex, 2=LAD'], inplace=True)\n",
    "    if outcome in feature_set:\n",
    "        feature_set.remove(outcome)\n",
    "\n",
    "    path = r'C:/Users/33753/OneDrive - Numa Health/Documents - 8 - Medical & Data Science/6 - Théo/03 - Research/5 - Pathologies/6-Myocarde/'\n",
    "    path += today+'/'+'MACE 2yr'+'/'+name+'/'\n",
    "    pipeline_classification(df, feature_set, col_to_pred=outcome, cutoff=0, thresh=None, save=True, outdir=path, show=False,\n",
    "                                scoring='accuracy', loss_function=None, dict_models=None, dict_param_models=None, normalization_method=None,\n",
    "                                imputation_method='iterative', feature_selection_method='Voting', n_features=4, calcul='No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = ['PMN','EOS','LYMPH','immed_adapt_score','cortisol_struc','Thyroid relaunching corrected','genit_thyro','IML','IMP','adapt_ratio'\n",
    "                '6 month Stroke','Killip klasė (CHF)','heart rate and conduction disarrangement during hospitalization']\n",
    "df = data_1.loc[~data_1[outcome].isna()].copy()\n",
    "if 'Culprit vessel: 0=RCA, 1=Rt Circumflex, 2=LAD' in df.columns:\n",
    "    df.drop(columns = ['Culprit vessel: 0=RCA, 1=Rt Circumflex, 2=LAD'], inplace=True)\n",
    "\n",
    "path = r'C:/Users/33753/OneDrive - Numa Health/Documents - 8 - Medical & Data Science/6 - Théo/03 - Research/5 - Pathologies/6-Myocarde/'\n",
    "path += today+'/'+'MACE 2yr'+'/'+'final_features'+'/'\n",
    "pipeline_classification(df, final_features, col_to_pred=outcome, cutoff=0, thresh=None, save=True, outdir=path, show=False,\n",
    "                            scoring='accuracy', loss_function=None, dict_models=None, dict_param_models=None, normalization_method=None,\n",
    "                            imputation_method='iterative', feature_selection_method='Voting', n_features=6, calcul='No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_1.loc[~data_1[outcome].isna()].copy()\n",
    "df = fix_decimal_separator(df)\n",
    "df = convert_to_float(df)\n",
    "if 'Culprit vessel: 0=RCA, 1=Rt Circumflex, 2=LAD' in df.columns:\n",
    "    df.drop(columns = ['Culprit vessel: 0=RCA, 1=Rt Circumflex, 2=LAD'], inplace=True)\n",
    "    \n",
    "a = df.copy()\n",
    "a.fillna(0, inplace = True)\n",
    "X = a.drop(outcome, axis = 1)\n",
    "y = a.loc[:,outcome]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AMI with double model and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29952\\1439865966.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Loading the notebook with all the index\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mfind_first_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIndexes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Get the path to Index.ipynb\u001b[39;00m\n\u001b[0;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, path) \u001b[38;5;66;03m# Equivalent to : %run \"C:/Users/33753/Documents/Notebooks/2 - General/Indexes.ipynb\" \u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29952\\375536622.py:29\u001b[0m, in \u001b[0;36mfind_first_file\u001b[1;34m(pattern, root_dir)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fuzz\u001b[38;5;241m.\u001b[39mpartial_ratio(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, name)\u001b[38;5;241m.\u001b[39mlower(),pattern\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m100\u001b[39m:\n\u001b[0;32m     27\u001b[0m             list_path\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, name))\n\u001b[1;32m---> 29\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mlist_path\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     31\u001b[0m path \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# If on Windows, replace backslashes with slashes\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Importing the librairies and the already written functions that we will use later on\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m33753\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDocuments\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mMachine-Learning\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m1-Functions\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mList_Functions.ipynb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m ## List of general functions that we will need such as visualization, training...\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# %run \"2 - General\\List functions.ipynb\"   ## List of general functions that we will need such as visualization, training...\u001b[39;00m\n\u001b[0;32m      4\u001b[0m today \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:2364\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2362\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2363\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2364\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\magics\\execution.py:717\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    715\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m preserve_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    716\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m--> 717\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_execfile_ipy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;66;03m# Control the response to exit() calls made by the script being run\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:2870\u001b[0m, in \u001b[0;36mInteractiveShell.safe_execfile_ipy\u001b[1;34m(self, fname, shell_futures, raise_exceptions)\u001b[0m\n\u001b[0;32m   2868\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_cell(cell, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell_futures\u001b[38;5;241m=\u001b[39mshell_futures)\n\u001b[0;32m   2869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exceptions:\n\u001b[1;32m-> 2870\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[0;32m   2872\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:266\u001b[0m, in \u001b[0;36mExecutionResult.raise_error\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_before_exec\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29952\\1439865966.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Loading the notebook with all the index\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mfind_first_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIndexes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Get the path to Index.ipynb\u001b[39;00m\n\u001b[0;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, path) \u001b[38;5;66;03m# Equivalent to : %run \"C:/Users/33753/Documents/Notebooks/2 - General/Indexes.ipynb\" \u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29952\\375536622.py:29\u001b[0m, in \u001b[0;36mfind_first_file\u001b[1;34m(pattern, root_dir)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fuzz\u001b[38;5;241m.\u001b[39mpartial_ratio(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, name)\u001b[38;5;241m.\u001b[39mlower(),pattern\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m100\u001b[39m:\n\u001b[0;32m     27\u001b[0m             list_path\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, name))\n\u001b[1;32m---> 29\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mlist_path\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     31\u001b[0m path \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# If on Windows, replace backslashes with slashes\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "## Importing the librairies and the already written functions that we will use later on\n",
    "%run \"C:\\Users\\33753\\Documents\\Machine-Learning\\1-Functions\\List_Functions.ipynb\" ## List of general functions that we will need such as visualization, training...\n",
    "# %run \"2 - General\\List functions.ipynb\"   ## List of general functions that we will need such as visualization, training...\n",
    "today = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the path to save all the plot, datasets etc\n",
    "path = r'C:/Users/33753/OneDrive - Numa Health/Documents - 8 - Medical & Data Science/6 - Théo/03 - Research/05 - Pathologies/06-Myocarde/'\n",
    "path += today+'/'+'Test PCA'+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Covid dataset                  \n",
    "data = pd.read_excel(r'C:/Users/33753/OneDrive - Numa Health/Documents - 8 - Medical & Data Science/6 - Théo/01 - Data/6 - Myocarde/df_myocarde_230420_reduced.xlsx',\n",
    "                     )\n",
    "data.set_index('Unnamed: 0', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the name of all the useful features and the outcome\n",
    "AMI_features = ['RBC (I) e6', 'WBC e3 (I)skyriuje', 'PMN (%) I',\n",
    "       'LYMPH (%)I', 'Mono (%) I', 'EOS (%) I', 'Cortisol INDEX', 'Mono/Lym',\n",
    "       'G/T', 'Adaptation', 'BASO (%) I', 'PLT.(I)', 'Hb mg/dL (I) ',\n",
    "       'MCH (I)', 'Na (I)', '?? Troponin [Trop MAX]',\n",
    "       'All SERUM Cortisol Kortizolis-Serum', 'FT3 (I)', 'First TnI',\n",
    "       'Total Cholesterol', 'eGFR 1 (GFG)', 'eGFR 1 CAT 0 = >90',\n",
    "       'Cr 1 (mmol/l)', 'CRB', 'Hospitalisation Period (days)', 'Age',\n",
    "       'Age CAT <65 = 0;', 'Sex: 0=MALE, 1=FEMALE', 'STEMI/ non STEMI',\n",
    "       'STEMI/ non STEMI.1', 'Killip klasė (CHF)', 'BMI (kg/m2)',\n",
    "       'BMI CAT >30=1', 'Dyslipidemia', 'Hypertesion', 'Smoking', 'DM',\n",
    "       'Familial IHD', 'CABG', 'History of IHD', 'History of heart failue',\n",
    "       'previous PCI', 'Previous MI', 'Risk AGG',\n",
    "       'EF after 24 hr from PCI >=50 NL, <=45 is increased risk of CHF, Mortality',\n",
    "       'EF post PCI CAT; <=45 = 1',\n",
    "       'Culprit vessel: 0=RCA, 1=Rt Circumflex, 2=LAD',\n",
    "       'Number of diseased vessel',\n",
    "       'percentage of culpret vessel; >=50 significant if other physiologic variables are present',\n",
    "       'Coronary dominancy (the artery that feeds the posterior descending artery): 40% RCA, 60% left Circumflex)',\n",
    "       'heart rate and conduction disarrangement during hospitalization',\n",
    "       'Cardiac rythm during incharge', 'in hospital AFib', 'SV during hosp. ',\n",
    "       'CRP', '6 month Stroke']\n",
    "outcome = 'MACE 2 yr (death, stroke, MI, hositalizacija(stento restenoz, new stenoz, SN))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary to rename all the biomarkers to a standard naming\n",
    "bio = {'RBC (I) e6': 'RBC',\n",
    " 'WBC e3 (I)skyriuje' : 'WBC',\n",
    " 'PMN (%) I' : 'PMN',\n",
    " 'LYMPH (%)I' : 'LYMPH',\n",
    " 'Mono (%) I' : 'MONO',\n",
    " 'EOS (%) I' : 'EOS',\n",
    " 'BASO (%) I' : 'BASO',\n",
    " 'PLT.(I)' : 'PLT',\n",
    " 'Hb mg/dL (I) ' : 'HB',\n",
    " 'HCT (I)' : 'HCT',\n",
    " 'MCH (I)' : 'MHC',\n",
    " 'MCHC (I)' : 'MCHC',\n",
    " 'K (I)' : 'K',\n",
    " 'Ca (I)' : 'CA'}\n",
    "\n",
    "data.rename(columns = bio, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the features to be removed according to the insight of Kamyar Hedayat\n",
    "toBeRemovedKamyar = ['STEMI/ non STEMI.1' ,'BMI (kg/m2)' ,'Dyslipidemia' ,'Hypertesion' ,'Smoking' ,\n",
    "        'DM' ,'CABG' ,'History of IHD' ,'History of heart failue' ,\n",
    "        'previous PCI' ,'Previous MI' ,'Risk AGG' ,'EF post PCI CAT; <=45 = 1' ,\n",
    "        'Culprit vessel: 0=RCA, 1=Rt Circumflex, 2=LAD' ,'Number of diseased vessel' ,'percentage of culpret vessel; >=50 significant if other physiologic variables are present' ,\n",
    "        'Coronary dominancy (the artery that feeds the posterior descending artery): 40% RCA, 60% left Circumflex)' ,\n",
    "        'Cardiac rythm during incharge','SV during hosp. ' ,'Time until death (1yr)/ Laikas iki mirties (1m)' ,\n",
    "        'Died in 1 yr / Mire per 1m.' ,'Time until death (2yr)/ Laikas iki mirties (2m)' ,'Na (I)' ,'eGFR 1 (GFG)' ,'Cr 1 (mmol/l)' ,'Cortisol INDEX' ,\n",
    "        'Death in hospital period; 0=No, 1=Yes' ,'1yr MACE( death. MI. stroke, hospitalisation)' ,'MACE 03.28']\n",
    "data.drop(columns=toBeRemovedKamyar, inplace=True)\n",
    "\n",
    "# List of features to removed to avoid duplicates\n",
    "data.drop(columns = ['Unnamed: 0.1','Mono/Lym','G/T','Adaptation'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing of the data\n",
    "df, new_feature_set = engineering(data, AMI_features, col_to_pred=outcome, cutoff=0, normalization_method=None, imputation_method='iterative',\n",
    "                                    feature_selection_method = None, n_features=5, calcul='Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X : the dataset without the expected outcome\n",
    "X = df.copy()\n",
    "X.drop(columns=[outcome], inplace=True)\n",
    "X_index = X.index\n",
    "\n",
    "# PCA\n",
    "# Apply PCA to reduce the dimensionality of the data to n_features\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pca.explained_variance_ : \\n\", pca.explained_variance_, '\\n')\n",
    "print(\"pca.explained_variance_ratio_ :\\n\", pca.explained_variance_ratio_, '\\n')\n",
    "print(\"pca.explained_variance_ratio_.cumsum() :\\n\", pca.explained_variance_ratio_.cumsum(), '\\n')\n",
    "print(\"pca.noise_variance_ :\\n\", pca.noise_variance_, '\\n')\n",
    "print(\"pca.singular_values_ :\\n\", pca.singular_values_, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the explained variance ratio cumulated with lines to better view the 90% threshold\n",
    "plt.barh(y = np.arange(1, len(list(pca.explained_variance_ratio_.cumsum()))+1), width = list(pca.explained_variance_ratio_.cumsum()), )\n",
    "plt.axhline(1, color = 'red')\n",
    "plt.axvline(0.90, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of facotrial axes to keep\n",
    "nbFactAxe = len([x for x in list(pca.explained_variance_ratio_.cumsum()) if x < 0.90]) + 1\n",
    "if nbFactAxe < 5:\n",
    "    nbFactAxe = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the most important features from the PCA and saving them\n",
    "mostImportantFeatures = pd.DataFrame()\n",
    "i=1\n",
    "index = range(len(pca.components_[0]))\n",
    "for comp in range(nbFactAxe):\n",
    "    s = sorted(index, reverse=True, key=lambda i: abs(pca.components_[comp])[i])[:nbFactAxe]\n",
    "    S = sorted(abs(pca.components_[comp]), reverse=True)[:nbFactAxe]\n",
    "    # print(S)\n",
    "    # print(s[:6])\n",
    "    mostImportantFeatures[i] = list(df.columns[s])\n",
    "    i+=1\n",
    "save_dataframe(outdir = path, dataframe=mostImportantFeatures, file_type='xlsx', name = 'Most important features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the most recurrent features\n",
    "value_counts = mostImportantFeatures.values.flatten().tolist()\n",
    "count = pd.Series(value_counts).value_counts()\n",
    "mostReccurentFeatures = pd.DataFrame(count)\n",
    "save_dataframe(outdir = path, dataframe=mostReccurentFeatures, file_type='xlsx', name = 'Occurences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first new variable is explaining more than 90% of the variance of the data which means that the first variable have more than 90% of the information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's only keep the 11 first variables\n",
    "fact_ax = list(np.arange(0,nbFactAxe))\n",
    "dfReduced = pd.DataFrame(X).loc[:,fact_ax]\n",
    "dfReduced['index']=X_index\n",
    "dfReduced = dfReduced.set_index('index')\n",
    "dfReduced[outcome] = df.loc[:,outcome]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization des données après le preprocessing\n",
    "vizualisation(dfReduced, fact_ax, outcome, save=True, outdir=path+'After_Preprocessing/', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "X, y = dfReduced.loc[:,list(dfReduced.columns[:-1])], dfReduced.loc[:,outcome]\n",
    "\n",
    "# Fitting the Decision Tree\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model = dt_model.fit(X,y)\n",
    "\n",
    "# Fitting the Fandom Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model = rf_model.fit(X,y)\n",
    "\n",
    "# Fitting the XGBoost\n",
    "xgb_model = XGBClassifier(eval_metric='mlogloss')\n",
    "xgb_model = xgb_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the results\n",
    "dict_result = {'DecisionTree':{},\n",
    "               'RandomForest':{},\n",
    "               'XGBoost':{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the variable that will store the model trained on the survival population\n",
    "dt_model_survival = DecisionTreeClassifier()\n",
    "rf_model_survival = RandomForestClassifier()\n",
    "xgb_model_survival = XGBClassifier(eval_metric='mlogloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StratifiedKFold with 10 splits\n",
    "skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize an empty list to store the residual errors\n",
    "residual_errors_dt = []\n",
    "residual_errors_rf = []\n",
    "residual_errors_xgb = []\n",
    "\n",
    "# Empty dataframe to store the wrong predictions\n",
    "dict_error_analysis = {'DecisionTree':[],\n",
    "               'RandomForest':[],\n",
    "               'XGBoost':[]}\n",
    "\n",
    "iter = 1\n",
    "\n",
    "# Iterate over each fold\n",
    "for train_index, test_index in skfold.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Showing the distribution of death and survival in each iteration\n",
    "    display_distribution_pie(df.iloc[train_index], outcome, save = True,\n",
    "                             outdir=path+'iter_'+str(iter)+'/', show=False)\n",
    "    display_distribution_pie(df.iloc[test_index], outcome, save = True,\n",
    "                             outdir=path+'iter_'+str(iter)+'/', show=False)\n",
    "\n",
    "    # Initialize and fit the decision tree model\n",
    "    dt_model_CV = DecisionTreeClassifier()\n",
    "    rf_model_CV = RandomForestClassifier()\n",
    "    xgb_model_CV = XGBClassifier(eval_metric='mlogloss')\n",
    "    \n",
    "    dt_model_CV = dt_model_CV.fit(X_train, y_train)\n",
    "    rf_model_CV = rf_model_CV.fit(X_train, y_train)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        xgb_model_CV = xgb_model_CV.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_dt = dt_model_CV.predict(X_test)\n",
    "    y_pred_rf = rf_model_CV.predict(X_test)\n",
    "    y_pred_xgb = xgb_model_CV.predict(X_test)\n",
    "\n",
    "\n",
    "    # Training a second model on the survival population to determine if a hidden pattern exists\n",
    "    X_train_survival = X_test.copy()\n",
    "    X_train_survival['Outcome'] = y_test\n",
    "\n",
    "    X_train_survival['PredictedOutcomeDt'] = y_pred_dt\n",
    "    X_train_survival['PredictedOutcomeRf'] = y_pred_rf\n",
    "    X_train_survival['PredictedOutcomeXgb'] = y_pred_xgb\n",
    "\n",
    "    X_train_survival_red = X_train_survival.loc[X_train_survival['Outcome']==0]\n",
    "    y_test_survival_dt = X_train_survival_red['PredictedOutcomeDt']\n",
    "    y_test_survival_rf = X_train_survival_red['PredictedOutcomeRf']\n",
    "    y_test_survival_xgb = X_train_survival_red['PredictedOutcomeXgb']\n",
    "\n",
    "    dt_model_survival = dt_model_survival.fit(X_train_survival_red.loc[:,fact_ax], y_test_survival_dt)\n",
    "    rf_model_survival = rf_model_survival.fit(X_train_survival_red.loc[:,fact_ax], y_test_survival_rf)\n",
    "    xgb_model_survival = xgb_model_survival.fit(X_train_survival_red.loc[:,fact_ax], y_test_survival_xgb)\n",
    "\n",
    "\n",
    "    # Adding the index of the wrong predictions to the corresponding dataframe\n",
    "    dict_error_analysis['DecisionTree'] += list(X_train_survival[X_train_survival['Outcome'] != X_train_survival['PredictedOutcomeDt']].index)\n",
    "    dict_error_analysis['RandomForest'] += list(X_train_survival[X_train_survival['Outcome'] != X_train_survival['PredictedOutcomeRf']].index)\n",
    "    dict_error_analysis['XGBoost'] += list(X_train_survival[X_train_survival['Outcome'] != X_train_survival['PredictedOutcomeXgb']].index)\n",
    "\n",
    "    # Calculate the residual error (e.g., mean squared error)\n",
    "    residual_error_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "    residual_errors_dt.append(round(residual_error_dt, 4))\n",
    "\n",
    "    residual_error_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "    residual_errors_rf.append(round(residual_error_rf,4))\n",
    "\n",
    "    residual_error_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "    residual_errors_xgb.append(round(residual_error_xgb,4))\n",
    "\n",
    "    # Plotting the confusion matrix for each model\n",
    "    metrics_dt = display_confusion_matrix(X_test, y_test, dt_model_CV, name=None, y_pred=y_pred_dt, return_metrics=True, save = True,\n",
    "                             outdir=path+'iter_'+str(iter)+'/', show=False)\n",
    "    metrics_rf = display_confusion_matrix(X_test, y_test, rf_model_CV, name=None, y_pred=y_pred_rf, return_metrics=True, save = True,\n",
    "                             outdir=path+'iter_'+str(iter)+'/', show=False)\n",
    "    metrics_xgb = display_confusion_matrix(X_test, y_test, y_pred_xgb, name=None, y_pred=y_pred_xgb, return_metrics=True, save = True,\n",
    "                             outdir=path+'iter_'+str(iter)+'/', show=False)\n",
    "    \n",
    "    # Saving the results in the dictionary\n",
    "    dict_result['DecisionTree'][str(iter)] = {'MSE' : round(residual_error_dt,4),\n",
    "                                    'SB' : round(metrics_dt[1],4),\n",
    "                                    'ST' : round(metrics_dt[2],4)}\n",
    "    dict_result['RandomForest'][str(iter)] = {'MSE' : round(residual_error_rf,4),\n",
    "                                    'SB' : round(metrics_rf[1],4),\n",
    "                                    'ST' : round(metrics_rf[2],4)}\n",
    "    dict_result['XGBoost'][str(iter)] = {'MSE' : round(residual_error_xgb,4),\n",
    "                                    'SB' : round(metrics_xgb[1],4),\n",
    "                                    'ST' : round(metrics_xgb[2],4)}\n",
    "\n",
    "    iter+=1\n",
    "\n",
    "# Print the residual error for the current fold and the average residual error across all folds\n",
    "print(\"Residual error (fold):\", residual_errors_dt)\n",
    "print(\"Average residual error:\", round(sum(residual_errors_dt) / len(residual_errors_dt),4))\n",
    "\n",
    "# Print the residual error for the current fold and the average residual error across all folds\n",
    "print(\"Residual error (fold):\", residual_errors_rf)\n",
    "print(\"Average residual error:\", round(sum(residual_errors_rf) / len(residual_errors_rf),4))\n",
    "\n",
    "# Print the residual error for the current fold and the average residual error across all folds\n",
    "print(\"Residual error (fold):\", residual_errors_xgb)\n",
    "print(\"Average residual error:\", round(sum(residual_errors_xgb) / len(residual_errors_xgb),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the rows that were falsely predicted\n",
    "save_dataframe(outdir=path+'Error analysis/', dataframe=data.loc[dict_error_analysis['DecisionTree'],:], file_type = 'xlsx', name='Error_Analysis_CV_dt')\n",
    "save_dataframe(outdir=path+'Error analysis/', dataframe=data.loc[dict_error_analysis['RandomForest'],:], file_type = 'xlsx', name='Error_Analysis_CV_rf')\n",
    "save_dataframe(outdir=path+'Error analysis/', dataframe=data.loc[dict_error_analysis['XGBoost'],:], file_type = 'xlsx', name='Error_Analysis_CV_xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean value and the STD of all the metrics\n",
    "df_AVG = pd.DataFrame()\n",
    "for typeModel in ['DecisionTree','RandomForest','XGBoost']:\n",
    "    MSE_list = []\n",
    "    SB_list=[]\n",
    "    ST_list=[]\n",
    "    for value in dict_result[typeModel].values():\n",
    "        MSE_list.append(value['MSE'])\n",
    "        SB_list.append(value['SB'])\n",
    "        ST_list.append(value['ST'])\n",
    "    df_AVG.loc['AVG',typeModel] = str({'MSE':round(np.mean(MSE_list),4),\n",
    "                                'SB':round(np.mean(SB_list),4),\n",
    "                                'ST':round(np.mean(ST_list),4)})\n",
    "    df_AVG.loc['STD',typeModel] = str({'MSE':round(np.std(MSE_list),4),\n",
    "                                'SB':round(np.std(SB_list),4),\n",
    "                                'ST':round(np.std(ST_list),4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results as a dataframe\n",
    "df_result = pd.DataFrame(dict_result)\n",
    "df_result = pd.concat([df_result,df_AVG])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LeaveOneOUt cross validation\n",
    "skfold = LeaveOneOut()\n",
    "\n",
    "# Initialize an empty list to store the residual errors\n",
    "residual_errors_dt = []\n",
    "residual_errors_rf = []\n",
    "residual_errors_xgb = []\n",
    "\n",
    "dict_PredictedOutcome = {'DecisionTree':[],\n",
    "                        'RandomForest':[],\n",
    "                        'XGBoost':[]}\n",
    "\n",
    "pred_test = pd.DataFrame()\n",
    "dict_error_analysis = {'DecisionTree':[],\n",
    "               'RandomForest':[],\n",
    "               'XGBoost':[]}\n",
    "\n",
    "# Iterate over each fold\n",
    "for train_index, test_index in skfold.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Initialize and fit the decision tree model\n",
    "    dt_model_CV = DecisionTreeClassifier()\n",
    "    rf_model_CV = RandomForestClassifier()\n",
    "    xgb_model_CV = XGBClassifier(eval_metric='mlogloss')\n",
    "    \n",
    "    dt_model_CV = dt_model_CV.fit(X_train, y_train)\n",
    "    rf_model_CV = rf_model_CV.fit(X_train, y_train)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        xgb_model_CV = xgb_model_CV.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_dt = dt_model_CV.predict(X_test)\n",
    "    dict_PredictedOutcome['DecisionTree'].append(y_pred_dt)\n",
    "    y_pred_rf = rf_model_CV.predict(X_test)\n",
    "    dict_PredictedOutcome['RandomForest'].append(y_pred_rf)\n",
    "    y_pred_xgb = xgb_model_CV.predict(X_test)\n",
    "    dict_PredictedOutcome['XGBoost'].append(y_pred_xgb)\n",
    "    pred_test = pd.concat([pred_test,y_test])\n",
    "\n",
    "    # Calculate the residual error (e.g., mean squared error)\n",
    "    residual_error_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "    residual_errors_dt.append(residual_error_dt)\n",
    "\n",
    "    residual_error_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "    residual_errors_rf.append(residual_error_rf)\n",
    "\n",
    "    residual_error_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "    residual_errors_xgb.append(residual_error_xgb)\n",
    "\n",
    "pred_test.loc[:,'PredictedOutcomeDt'] = [int(arr[0]) for arr in dict_PredictedOutcome['DecisionTree']]\n",
    "pred_test.loc[:,'PredictedOutcomeRf'] = [int(arr[0]) for arr in dict_PredictedOutcome['RandomForest']]\n",
    "pred_test.loc[:,'PredictedOutcomeXgb'] = [int(arr[0]) for arr in dict_PredictedOutcome['XGBoost']]\n",
    "\n",
    "# Adding the index of the wrong predictions to the corresponding dataframe\n",
    "dict_error_analysis['DecisionTree'] += list(pred_test[pred_test[0] != pred_test['PredictedOutcomeDt']].index)\n",
    "dict_error_analysis['RandomForest'] += list(pred_test[pred_test[0] != pred_test['PredictedOutcomeRf']].index)\n",
    "dict_error_analysis['XGBoost'] += list(pred_test[pred_test[0] != pred_test['PredictedOutcomeXgb']].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the rows that were falsely predicted\n",
    "save_dataframe(outdir=path+'Error analysis/', dataframe=data.loc[dict_error_analysis['DecisionTree'],:], file_type = 'xlsx', name='Error_Analysis_LOO_dt')\n",
    "save_dataframe(outdir=path+'Error analysis/', dataframe=data.loc[dict_error_analysis['RandomForest'],:], file_type = 'xlsx', name='Error_Analysis_LOO_rf')\n",
    "save_dataframe(outdir=path+'Error analysis/', dataframe=data.loc[dict_error_analysis['XGBoost'],:], file_type = 'xlsx', name='Error_Analysis_LOO_xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LOO = pd.DataFrame()\n",
    "for typeModel in ['DecisionTree','RandomForest','XGBoost']:\n",
    "\n",
    "    # Calculate the confusion matrix for all folds\n",
    "    cm = confusion_matrix(pred_test[0], dict_PredictedOutcome[typeModel])\n",
    "\n",
    "    # Calculate sensitivity (recall)\n",
    "    sensitivity = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
    "\n",
    "    # Calculate specificity\n",
    "    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    mse = mean_squared_error(pred_test[0], dict_PredictedOutcome[typeModel])\n",
    "\n",
    "    df_LOO.loc['LOO',typeModel] = str({'MSE':round(mse,4),\n",
    "                                     'SB':round(sensitivity,4),\n",
    "                                     'ST':round(specificity,4)})\n",
    "\n",
    "    print(\"Sensitivity:\", round(sensitivity,4))\n",
    "    print(\"Specificity:\", round(specificity,4))\n",
    "    print(\"Mean Squared Error:\", round(mse,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe with all the results\n",
    "df_result = pd.concat([df_result,df_LOO])\n",
    "df_result = df_result.applymap(lambda x: str(x).replace('{', '').replace('}', ''))\n",
    "save_dataframe(outdir=path, dataframe=df_result, file_type = 'xlsx', name='results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Perform PCA on the dataset\n",
    "# data = pd.read_csv('your_dataset.csv')  # Load your dataset\n",
    "# X = data.drop('class', axis=1)  # Extract the feature columns\n",
    "# y = data['class']  # Extract the class column\n",
    "\n",
    "# pca = PCA(n_components=X.shape[1])\n",
    "# X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Step 2: Extract the first 3 principal components\n",
    "principal_components = X.loc[:,:3]\n",
    "principal_components.reset_index(inplace=True)\n",
    "\n",
    "# Step 3: Create a scatter plot to visualize the individuals\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Color code the data points based on the class\n",
    "colors = {0.0: 'red', 1.0: 'blue'}\n",
    "for i, target in enumerate(np.unique(y)):\n",
    "    indices = np.where(y == target)\n",
    "    ax.scatter(principal_components.loc[indices, 0],\n",
    "               principal_components.loc[indices, 1],\n",
    "               principal_components.loc[indices, 2],\n",
    "               c=colors[target], label=target)\n",
    "\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat = data.describe()\n",
    "save_dataframe(outdir=path, dataframe=df_stat, file_type = 'xlsx', name='Description des données')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END of PCA study"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
